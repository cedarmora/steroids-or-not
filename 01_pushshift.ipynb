{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Loads environment variables from .env file\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp pushshift\n",
    "# all_local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect all nattyorjuice post urls to manually gather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install/import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "from requests.adapters import HTTPAdapter\n",
    "import webbrowser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data discovery/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pushshift = \"https://api.pushshift.io/reddit/\"\n",
    "pushshift_submission_url = pushshift + \"submission/search\"\n",
    "one_year_seconds = 1 * 365 * 24 * 60 * 60\n",
    "before = int(time.time() - (one_year_seconds * 9))\n",
    "\n",
    "params = {\n",
    "    \"subreddit\": \"nattyorjuice\",\n",
    "    \"size\": \"25\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(pushshift_submission_url, params=params)\n",
    "len(response.json()['data'])\n",
    "response.json()['data'][0]\n",
    "response.json()['data'][0]\n",
    "response.json()['data'][0]['permalink']\n",
    "response.json()['data'][0]['full_link']\n",
    "response.json()['data'][0]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(len(response.json()['data']), 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling/ Downloading Data from pushshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubredditCrawler(object):\n",
    "    \"\"\"\n",
    "    Borrowed heavily from here: https://www.textjuicer.com/2019/07/crawling-all-submissions-from-a-subreddit/\n",
    "    \"\"\"\n",
    "\n",
    "    pushshift = \"https://api.pushshift.io/reddit/\"\n",
    "    pushshift_submission_url = pushshift + \"submission/search\"\n",
    "\n",
    "    def __init__(self, subreddit, file_path, max_submissions=200):\n",
    "        self.subreddit = subreddit\n",
    "        self.file_path = file_path\n",
    "        self.max_submissions = max_submissions\n",
    "        self.submissions = []\n",
    "\n",
    "        self.session = requests.Session()\n",
    "        self.session.mount('http://api.pushshift.io/', HTTPAdapter(max_retries=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubredditCrawler(SubredditCrawler):\n",
    "    def crawl_page(self, last_page=None):\n",
    "        \"\"\"\n",
    "        Crawl a page of results from a given subreddit.\n",
    "\n",
    "        :param subreddit: The subreddit to crawl.\n",
    "        :param last_page: The last downloaded page.\n",
    "\n",
    "        :return: A page or results.\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            \"subreddit\": self.subreddit,\n",
    "            \"size\": 100,\n",
    "            \"sort\": \"desc\",\n",
    "            \"sort_type\": \"created_utc\",\n",
    "        }\n",
    "        if last_page is not None:\n",
    "            if len(last_page) > 0:\n",
    "                # resume from where we left at the last page\n",
    "                params[\"before\"] = last_page[-1][\"created_utc\"]\n",
    "            else:\n",
    "                # the last page was empty, we are past the last page\n",
    "                return []\n",
    "        results = self.session.get(pushshift_submission_url, params=params)\n",
    "        \n",
    "        if not results.ok:\n",
    "            # something wrong happened\n",
    "            raise Exception(\n",
    "                \"Server returned status code {}\".format(results.status_code)\n",
    "            )\n",
    "        return results.json()[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubredditCrawler(SubredditCrawler):\n",
    "    def crawl_subreddit(self):\n",
    "        \"\"\"\n",
    "        Crawl submissions from a subreddit.\n",
    "        Isn't strictly correct on the number of submissions it grabs but doesn't matter too much\n",
    "        for our purposes\n",
    "\n",
    "        :param subreddit: The subreddit to crawl.\n",
    "        :param max_submissions: The maximum number of submissions to download.\n",
    "\n",
    "        :return: A list of submissions.\n",
    "        \"\"\"\n",
    "        last_page = None\n",
    "\n",
    "        print(f\"Started {datetime.datetime.now()}\")\n",
    "\n",
    "        with open(self.file_path, \"wb\") as file:\n",
    "\n",
    "            while last_page != [] and len(self.submissions) < self.max_submissions:\n",
    "                last_page = self.crawl_page(last_page)\n",
    "\n",
    "                self.submissions += last_page\n",
    "                pickle.dump(last_page, file)\n",
    "                print(f\"---- pickled {len(self.submissions)} posts so far ------\")\n",
    "                print(f\"Last post title: {last_page[-1]['title']}\")\n",
    "\n",
    "                time.sleep(3)\n",
    "\n",
    "        print(f\"--------------------------------------\")\n",
    "        print(f\"Finished {datetime.datetime.now()}\")\n",
    "        print(f\"PICKLED {len(self.submissions)} SUBMISSIONS to file:\")\n",
    "        print(f\"{self.file_path}\")\n",
    "        print(f\"--------------------------------------\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Important to keep these two lines in the same cell so old data doesn't\n",
    "# get overwritten\n",
    "submissions_path = f'data/submissions.{int(time.time())}.pkl'\n",
    "crawler = SubredditCrawler('nattyorjuice', submissions_path, 80000)\n",
    "# 42000 posts took about 40 minutes\n",
    "\n",
    "crawler.crawl_subreddit()\n",
    "len(crawler.submissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important to keep these two lines in the same cell so old data doesn't\n",
    "# get overwritten\n",
    "submissions_path = f'data/submissions.{int(time.time())}.pkl'\n",
    "crawler = SubredditCrawler('nattyorjuice', submissions_path, 100)\n",
    "\n",
    "crawler.crawl_subreddit()\n",
    "assert len(crawler.submissions) == 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_list(urls):\n",
    "    '''\n",
    "    Opens a list of urls in web browser for convenience in viewing data\n",
    "    '''\n",
    "    [webbrowser.open(url) for url in urls]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
