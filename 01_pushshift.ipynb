{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp pushshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect all nattyorjuice post urls to manually gather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install/import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "!pip install .[dev]\n",
    "\n",
    "#!jupyter nbextension install https://github.com/drillan/jupyter-black/archive/master.zip --user\n",
    "#!jupyter nbextension enable jupyter-black-master/jupyter-black\n",
    "\n",
    "!nbdime extensions --enable --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "# Loads environment variables from .env file\n",
    "\n",
    "import os\n",
    "import praw\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "import pickle\n",
    "from requests.adapters import HTTPAdapter\n",
    "from collections import Counter\n",
    "import webbrowser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRAW stuff\n",
    "client_id = os.environ.get('REDDIT_CLIENT_ID')\n",
    "client_secret = os.environ.get('REDDIT_CLIENT_SECRET')\n",
    "user_agent = 'User-Agent: Steroid detector bot by /u/thetreecycle'\n",
    "\n",
    "# Output client_id to see if it's working\n",
    "client_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pushshift = \"https://api.pushshift.io/reddit/\"\n",
    "pushshift_submission_url = pushshift + \"submission/search\"\n",
    "one_year_seconds = 1 * 365 * 24 * 60 * 60\n",
    "before = int(time.time() - (one_year_seconds * 9))\n",
    "\n",
    "params = {\n",
    "    \"subreddit\": \"nattyorjuice\",\n",
    "    \"size\": \"25\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(pushshift_submission_url, params=params)\n",
    "len(response.json()['data'])\n",
    "response.json()['data'][0]\n",
    "response.json()['data'][0]\n",
    "response.json()['data'][0]['permalink']\n",
    "response.json()['data'][0]['full_link']\n",
    "response.json()['data'][0]['url']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if response.status_code == 200:\n",
    "    with open(natural_path / filename, 'wb') as f:\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling/ Downloading Data from pushshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubredditCrawler(object):\n",
    "    \"\"\"\n",
    "    Borrowed heavily from here: https://www.textjuicer.com/2019/07/crawling-all-submissions-from-a-subreddit/\n",
    "    \"\"\"\n",
    "\n",
    "    pushshift = \"https://api.pushshift.io/reddit/\"\n",
    "    pushshift_submission_url = pushshift + \"submission/search\"\n",
    "\n",
    "    def __init__(self, subreddit, file_path, max_submissions=200):\n",
    "        self.subreddit = subreddit\n",
    "        self.file_path = file_path\n",
    "        self.max_submissions = max_submissions\n",
    "        self.submissions = []\n",
    "\n",
    "        self.session = requests.Session()\n",
    "        self.session.mount('http://api.pushshift.io/', HTTPAdapter(max_retries=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubredditCrawler(SubredditCrawler):\n",
    "    def crawl_page(self, last_page=None):\n",
    "        \"\"\"\n",
    "        Crawl a page of results from a given subreddit.\n",
    "\n",
    "        :param subreddit: The subreddit to crawl.\n",
    "        :param last_page: The last downloaded page.\n",
    "\n",
    "        :return: A page or results.\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            \"subreddit\": self.subreddit,\n",
    "            \"size\": 100,\n",
    "            \"sort\": \"desc\",\n",
    "            \"sort_type\": \"created_utc\",\n",
    "        }\n",
    "        if last_page is not None:\n",
    "            if len(last_page) > 0:\n",
    "                # resume from where we left at the last page\n",
    "                params[\"before\"] = last_page[-1][\"created_utc\"]\n",
    "            else:\n",
    "                # the last page was empty, we are past the last page\n",
    "                return []\n",
    "        results = self.session.get(pushshift_submission_url, params=params)\n",
    "        \n",
    "        if not results.ok:\n",
    "            # something wrong happened\n",
    "            raise Exception(\n",
    "                \"Server returned status code {}\".format(results.status_code)\n",
    "            )\n",
    "        return results.json()[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubredditCrawler(SubredditCrawler):\n",
    "    def crawl_subreddit(self, erase_self_submissions=False):\n",
    "        \"\"\"\n",
    "        Crawl submissions from a subreddit.\n",
    "        Isn't strictly correct on the number of submissions it grabs but doesn't matter too much\n",
    "        for our purposes\n",
    "\n",
    "        :param subreddit: The subreddit to crawl.\n",
    "        :param max_submissions: The maximum number of submissions to download.\n",
    "\n",
    "        :return: A list of submissions.\n",
    "        \"\"\"\n",
    "        last_page = None\n",
    "\n",
    "        if erase_self_submissions:\n",
    "            self.submissions = []\n",
    "\n",
    "        print(f\"Started {datetime.datetime.now()}\")\n",
    "\n",
    "        with open(self.file_path, \"wb\") as file:\n",
    "\n",
    "            while last_page != [] and len(self.submissions) < self.max_submissions:\n",
    "                last_page = self.crawl_page(last_page)\n",
    "\n",
    "                self.submissions += last_page\n",
    "                pickle.dump(last_page, file)\n",
    "                print(f\"---- pickled {len(self.submissions)} posts so far ------\")\n",
    "                print(f\"Last post title: {last_page[-1]['title']}\")\n",
    "\n",
    "                time.sleep(3)\n",
    "\n",
    "        print(f\"--------------------------------------\")\n",
    "        print(f\"Finished {datetime.datetime.now()}\")\n",
    "        print(f\"PICKLED {len(self.submissions)} SUBMISSIONS to file:\")\n",
    "        print(f\"{self.file_path}\")\n",
    "        print(f\"--------------------------------------\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Important to keep these two lines in the same cell so old data doesn't\n",
    "# get overwritten\n",
    "submissions_path = f'data/submissions.{int(time.time())}.pkl'\n",
    "crawler = SubredditCrawler('nattyorjuice', submissions_path, 80000)\n",
    "# 42000 posts took about 40 minutes\n",
    "\n",
    "crawler.crawl_subreddit()\n",
    "len(crawler.submissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_list(urls):\n",
    "    '''\n",
    "    Opens a list of urls in web browser for convenience in viewing data\n",
    "    '''\n",
    "    [webbrowser.open(url) for url in urls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpickling, cleaning downloaded pushshift data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def unpickle(path):\n",
    "    pickled_list = []\n",
    "    with open(path, \"rb\") as file:\n",
    "        while 1:\n",
    "            try:\n",
    "                pickled_list += pickle.load(file)\n",
    "            except EOFError:\n",
    "                break\n",
    "\n",
    "    print(f'Unpickled {len(pickled_list)} objects.')\n",
    "    return pickled_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpickled 41725 posts.\n"
     ]
    }
   ],
   "source": [
    "pickled_posts = unpickle('data/all_nattyorjuice_submissions.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common post link domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([post['domain'] for post in pickled_posts]).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove irrelevant posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def select_mostly_useful(posts):\n",
    "    '''\n",
    "    Remove mostly invalid posts, stuff like:\n",
    "    few comments (so no label),\n",
    "    no selftext are often not useful\n",
    "    post was deleted or removed\n",
    "    \n",
    "    It's not a complete solution, as the pushshift data\n",
    "    doesn't get updated when the post changes, so there\n",
    "    are many posts which were deleted after fact.\n",
    "    '''\n",
    "    mostly_useful = []\n",
    "    for post in posts:\n",
    "        if (post['num_comments'] >= 2 and\n",
    "                'selftext' in post and\n",
    "                post['selftext'] != '[deleted]' and\n",
    "                post['selftext'] != '[removed]' and\n",
    "                post.get('removed_by_category') == None and\n",
    "                post.get('link_flair_text') != 'Meme' and\n",
    "                post.get('domain') == 'i.redd.it'):\n",
    "            mostly_useful.append(post)\n",
    "    return mostly_useful\n",
    "    \n",
    "    \n",
    "# 34258"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostly_useful = select_mostly_useful(pickled_posts)\n",
    "len(mostly_useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selftext = [post['selftext'] for post in mostly_useful]\n",
    "len(selftext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show that it's only image data\n",
    "Counter(selftext).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate how to exclude deleted posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent,\n",
    ")\n",
    "\n",
    "nattyorjuice = reddit.subreddit('nattyorjuice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = {\n",
    "    'deleted': reddit.submission(url='https://www.reddit.com/r/nattyorjuice/comments/oxsr5t/kenneth_the_ii_is_he_natty_or_juice/'),\n",
    "\n",
    "    'mod_removed': reddit.submission(url='https://www.reddit.com/r/nattyorjuice/comments/owvbpl/chul_soon/'),\n",
    "\n",
    "    'normal': reddit.submission(url='https://www.reddit.com/r/nattyorjuice/comments/p17okr/how_long_does_it_take_to_get_this_natty_physique/'),\n",
    "\n",
    "    'spam_removed': reddit.submission(url='https://www.reddit.com/r/nattyorjuice/comments/oqwkmd/55_230lbs_what_do_we_think/'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(posts['deleted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(posts['mod_removed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(posts['normal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(posts['spam_removed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the `removed_by_category` field on submissions is `None`, the submission has not been removed or deleted \n",
    "\n",
    "P.S. usually? The selftext is sometimes set to [removed] or [deleted] without the `removed_by_category` being set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
