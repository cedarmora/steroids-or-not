{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect all nattyorjuice post urls to manually gather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install/import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "!pip install .[dev]\n",
    "\n",
    "#!jupyter nbextension install https://github.com/drillan/jupyter-black/archive/master.zip --user\n",
    "#!jupyter nbextension enable jupyter-black-master/jupyter-black\n",
    "\n",
    "!nbdime extensions --enable --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "\n",
    "## Loads environment variables from .env file\n",
    "\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "import pickle\n",
    "from requests.adapters import HTTPAdapter\n",
    "from collections import Counter\n",
    "import webbrowser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pushshift = \"https://api.pushshift.io/reddit/\"\n",
    "pushshift_submission_url = pushshift + \"submission/search\"\n",
    "one_year_seconds = 1 * 365 * 24 * 60 * 60\n",
    "before = int(time.time() - (one_year_seconds * 9))\n",
    "\n",
    "params = {\n",
    "    \"subreddit\": \"nattyorjuice\",\n",
    "    \"size\": \"25\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t0 = time.time()\n",
    "response = requests.get(pushshift_submission_url, params=params)\n",
    "# t1 = time.time()\n",
    "\n",
    "# total = t1-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(response.json()['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()['data'][0]['permalink']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()['data'][0]['full_link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()['data'][0]['url']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if response.status_code == 200:\n",
    "    with open(natural_path / filename, 'wb') as f:\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling/ Downloading Data from pushshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubredditCrawler(object):\n",
    "    \"\"\"\n",
    "    Borrowed heavily from here: https://www.textjuicer.com/2019/07/crawling-all-submissions-from-a-subreddit/\n",
    "    \"\"\"\n",
    "\n",
    "    pushshift = \"https://api.pushshift.io/reddit/\"\n",
    "    pushshift_submission_url = pushshift + \"submission/search\"\n",
    "\n",
    "    def __init__(self, subreddit, file_path, max_submissions=200):\n",
    "        self.subreddit = subreddit\n",
    "        self.file_path = file_path\n",
    "        self.max_submissions = max_submissions\n",
    "        self.submissions = []\n",
    "\n",
    "        self.session = requests.Session()\n",
    "        self.session.mount('http://api.pushshift.io/', HTTPAdapter(max_retries=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubredditCrawler(SubredditCrawler):\n",
    "    def crawl_page(self, last_page=None):\n",
    "        \"\"\"\n",
    "        Crawl a page of results from a given subreddit.\n",
    "\n",
    "        :param subreddit: The subreddit to crawl.\n",
    "        :param last_page: The last downloaded page.\n",
    "\n",
    "        :return: A page or results.\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            \"subreddit\": self.subreddit,\n",
    "            \"size\": 100,\n",
    "            \"sort\": \"desc\",\n",
    "            \"sort_type\": \"created_utc\",\n",
    "        }\n",
    "        if last_page is not None:\n",
    "            if len(last_page) > 0:\n",
    "                # resume from where we left at the last page\n",
    "                params[\"before\"] = last_page[-1][\"created_utc\"]\n",
    "            else:\n",
    "                # the last page was empty, we are past the last page\n",
    "                return []\n",
    "        results = self.session.get(pushshift_submission_url, params=params)\n",
    "        \n",
    "        if not results.ok:\n",
    "            # something wrong happened\n",
    "            raise Exception(\n",
    "                \"Server returned status code {}\".format(results.status_code)\n",
    "            )\n",
    "        return results.json()[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubredditCrawler(SubredditCrawler):\n",
    "    def crawl_subreddit(self, erase_self_submissions=False):\n",
    "        \"\"\"\n",
    "        Crawl submissions from a subreddit.\n",
    "        Isn't strictly correct on the number of submissions it grabs but doesn't matter too much\n",
    "        for our purposes\n",
    "\n",
    "        :param subreddit: The subreddit to crawl.\n",
    "        :param max_submissions: The maximum number of submissions to download.\n",
    "\n",
    "        :return: A list of submissions.\n",
    "        \"\"\"\n",
    "        last_page = None\n",
    "\n",
    "        if erase_self_submissions:\n",
    "            self.submissions = []\n",
    "\n",
    "        print(f\"Started {datetime.datetime.now()}\")\n",
    "\n",
    "        with open(self.file_path, \"wb\") as file:\n",
    "\n",
    "            while last_page != [] and len(self.submissions) < self.max_submissions:\n",
    "                last_page = self.crawl_page(last_page)\n",
    "\n",
    "                self.submissions += last_page\n",
    "                pickle.dump(last_page, file)\n",
    "                print(f\"---- pickled {len(self.submissions)} posts so far ------\")\n",
    "                print(f\"Last post title: {last_page[-1]['title']}\")\n",
    "\n",
    "                time.sleep(3)\n",
    "\n",
    "        print(f\"--------------------------------------\")\n",
    "        print(f\"Finished {datetime.datetime.now()}\")\n",
    "        print(f\"PICKLED {len(self.submissions)} SUBMISSIONS to file:\")\n",
    "        print(f\"{self.file_path}\")\n",
    "        print(f\"--------------------------------------\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Important to keep these two lines in the same cell so old data doesn't\n",
    "# get overwritten\n",
    "submissions_path = f'data/submissions.{int(time.time())}.pkl'\n",
    "crawler = SubredditCrawler('nattyorjuice', submissions_path, 80000)\n",
    "# 42000 posts took about 40 minutes\n",
    "\n",
    "crawler.crawl_subreddit()\n",
    "len(crawler.submissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_list(urls):\n",
    "    '''\n",
    "    Opens a list of urls in web browser for convenience in viewing data\n",
    "    '''\n",
    "    [webbrowser.open(url) for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpickling, cleaning downloaded pushshift data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_posts = []\n",
    "with open('data/all_nattyorjuice_submissions.pkl', \"rb\") as file:\n",
    "    while 1:\n",
    "        try:\n",
    "            pickled_posts += pickle.load(file)\n",
    "        except EOFError:\n",
    "            break\n",
    "\n",
    "len(pickled_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove invalid posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostly_valid = [post for post in pickled_posts if post['num_comments'] >= 2]\n",
    "len(mostly_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common post link domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([post['domain'] for post in mostly_valid]).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i.redd.it posts AKA just the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_redd_it = [post for post in mostly_valid if post['domain'] == 'i.redd.it']\n",
    "len(i_redd_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove certain flair e.g. Meme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flaired = [post for post in i_redd_it if 'link_flair_text' in post]\n",
    "len(flaired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([post['link_flair_text'] for post in flaired]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_irrelevant_posts(posts):\n",
    "    relevant_posts = []\n",
    "    for post in posts:\n",
    "        if 'link_flair_text' in post:\n",
    "            flair = post['link_flair_text']\n",
    "            if flair == 'Meme':\n",
    "                # Memes are not useful for our purposes, exclude them\n",
    "                continue\n",
    "        # Include everything else\n",
    "        relevant_posts.append(post)\n",
    "    return relevant_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pictures_no_memes = exclude_irrelevant_posts(i_redd_it)\n",
    "len(pictures_no_memes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate how to exclude deleted posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
